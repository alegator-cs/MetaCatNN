The intent here is to define some class of neural networks called "OKNets" that map binary data to data of arbitrary dimension and interpretation based on differential synthesis from binary data to the output space of an arbitrary neural network. We will be defining a specific instance of an "OKNet" called "MetaCatNN."

This work was made possible by a GPT subscription. I have no suitable workplace or institution from which to present the work, but it wouldn't be possible with access to GPT4o, so it feels right to regard OpenAI as the institution at least somehow sponsoring this work, as well as other organizations such as X and GitHub making possible its publication, dissemination, and discussion. I am quite certain the amount of compute I've had access to is well in excess of my $20/mo subscription, to the point that it would be comedy to say otherwise.

Categorically, not one academic institution has in any way acted to encourage or support this work, save William Rainey Harper College, from which I have an associate's degree in science, solely for serving educational function as intended, not getting in my way, not acting to harm me, but rather (as intended) enabling me to thrive. The exact opposite has been the case in every other academic institution I have come into contact with, and I have no kind words for these institutions. I am held ransom by the loans I owe to these places that are somehow simultaneously places of abject stupidity yet also the pinnacles of public sector knowledge. I have been scammed by petty thugs. Help is much appreciated. I have been paying attention to the likes of Scott Galloway, Eric Weinstein, and Peter Thiel. I would like to note that I am utterly miserable in my present dispositions. Maybe they might also pay a bit of attention to me.

As a lemma, we require an extended definition of what a function can be, notably, we would like to make use of symbolic rewrites as in automata theory. In this case, we define a bijection between the cantor set and [0,1] as the rewrite rule where the ternary value's 1 digits are set to 0.

As another lemma, consider a novel limit definition of continuity with an arbitrary maximum distance parametrized on the bit depth (truncation) of the symbol rewriting rule. Regard the nominal difference of the mapped images of values, that metric is parametrized on an arbitrary maximum distance between the original and the mapped value. Thus we map a discontinuous infinitely uncountable set to a continuous infinitely uncountable set where the limit defining the aforementioned metric tends to zero as bit depth tends towards infinity.

We call it "OKCompression" to apply such a bijection parametrized on bit depth.

In other words, saying that the halting problem is intractable is as useful as saying that you cannot map discontinuous uncountably infinite sets to continuous uncountably infinite sets, and says just as much about the general thoughtfulness of a person making such a claim, while unironically using hardware dependent on solving the halting problem for the primitive recursive cases to disseminate such claims. No kind words, as already mentioned, for the institutions propagating such misleading facts, without proper discussion of the limit, towards the apparent end of general ill intent towards mankind, insofar as I can infer.

Now, to simplify the definition of MetaCatNN, we consider some CatCNN trained specificially and solely for cat image recognition. Train also CatCNN(d) over the integer range [0, d_max] where the input layer is now instead fed a mapped image parametrized by d=depth of cantor set dimensionality reduction (the "OKCompression"). Each CatCNN(d) can be trained by synthesizing a dataset from the training and validation sets used to train the CatCNN, by simply applying OKCompression(d) to the images in both sets, and then running the same training as for CatCNN. The output layer of any given CatCNN(d) has the same geometry as the input layer, so this allows chaining CatCNN(d) applications in order of increasing depth, where each CatCNN(d) is a layer in a meta neural network, call it MetaCatNN, whose input space is arbitrary binary data reshaped as some image w=width and h=height, with w and h held constant for the proceeding purposes (they may vary in more sophisticated variations of OKNet). Since w and h are held constant the network geometry is the same at each CatNN(d) layer. It may well be interesting to put different kinds of layers between each CatNN(d) layer, but for the proceeding purposes, let's regard that each CatNN(d) layer is fully connected to the CatNN(d-1) and CatNN(d+1) layers, except for the input and output layers. Thus we generate image output in the style of an arbitrary neural network from arbitrary binary data, and this approach of course generalizes to any data interpretation, not just images.

QED?